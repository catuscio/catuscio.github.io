---
categories:
- Studies
date: '2024-12-12'
tags:
- Lecture
- Data Analysis
- ë°ì´í„°ë¶„ì„
- PCA
title: '[Lecture] Feature Selection & Dimensionality Reduction'
toc: true
---

---


# Feature Selection

- ê³¼ì í•© ë¬¸ì œ í•´ê²°í•˜ê¸°
    - Biasì™€ VarianceëŠ” ìƒì¶© ê´€ê³„
    - ê³¼ì í•© ë¬¸ì œëŠ” Biasë¥¼ ì¤„ì—¬ë„ Varianceê°€ ë„ˆë¬´ ì»¤ì ¸ì„œ ë°œìƒ
    - Biasë¥¼ ì¡°ê¸ˆ í¬ê¸°í•´ë„ Varianceë¥¼ ì¤„ì¼ ìˆ˜ ìˆëŠ” ë°©ë²•?
- Feature ê°œìˆ˜ ì¤„ì´ê¸°
    - ì£¼ìš” íŠ¹ì§•ì„ ì„ íƒí•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ì‚¬ìš© X
    - Feature Selection Algorithm
- ì •ê·œí™” ìˆ˜í–‰ (Regularization)
    - ìš°ë¦¬ê°€ ì¶”ì •í•˜ëŠ” íŒŒë¼ë¯¸í„° ê°’ì˜ í¬ê¸°ë¥¼ ì¡°ì ˆí•´ë³´ì

### ì„ í˜• íšŒê·€ ëª¨ë¸ (ìµœì†Œì œê³±í•©)ì˜ ë¬¸ì œ

- ì˜ˆì¸¡ë ¥ (Prediction accuracy) : ìµœì†Œì œê³±í•©ì„ ì´ìš©í•˜ëŠ” íŒŒë¼ë¯¸í„° ì¶”ì •ì€ Biasë¥¼ ì¤„ì´ì§€ë§Œ Outlier ë“±ì— í¬ê²Œ ë¯¼ê°í•˜ë¯€ë¡œ Varianceê°€ í¬ë‹¤.
    - ë³€ìˆ˜ì˜ ê°œìˆ˜ë¥¼ ì¤„ì´ê±°ë‚˜ parameter í¬ê¸°ë¥¼ ì¤„ì—¬ì„œ í•´ê²°í•  ìˆœ ì—†ì„ê¹Œ?
â†’ ê²°êµ­ biasë¥¼ ì¡°ê¸ˆ í¬ìƒí•˜ë©´ì„œ variance ì¤„ì´ê¸°

- í•´ì„ ê°€ëŠ¥ì„± (Interpretability) : ëª¨ë¸ í•´ì„ë ¥ì„ ë†’ì´ê¸° ìœ„í•´ì„œëŠ” ë³´ë‹¤ ì ì€ feature ì‚¬ìš©í•´ì•¼

### Solution

- Best Subset Selection
    - íŠ¹ì„±ì´ pê°œ ìˆì„ ë•Œ, ëª¨ë¸ ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ì•„ì§€ëŠ” íŠ¹ì„± kê°œë§Œ ê³ ë¥´ì.
    - ì´ $$\frac{p!}{k!(p-k)!}$$ì˜ ë¶€ë¶„ì§‘í•© ìˆ˜ë¥¼ íƒìƒ‰í•´ì•¼ í•¨ (computationally expensive)
- Forward-stepwise selection (greedy algorithm)
    - ì ˆí¸ìœ¼ë¡œë¶€í„° ì‹œì‘í•˜ì—¬, ê°€ì¥ ì„±ëŠ¥ì„ ë†’ì´ëŠ” íŠ¹ì„±ì„ í•˜ë‚˜ì”© ì¶”ê°€
    - ì„±ëŠ¥ì´ ê°€ì¥ ë†’ì„ ë•Œ ì„ íƒ ì¢…ë£Œ
    - í†µê³„ì ìœ¼ë¡œ p-valueë¥¼ í™œìš©í•˜ì—¬ ì„ íƒ ê°€ëŠ¥

## Forward-stepwise selection (greedy algorithm)

<div class='column-list' style='display: flex; gap: 1em;'>
<div class='column' style='flex: 1; padding: 0.5em; background-color: #f9f9f9; border: 1px solid #ddd; border-radius: 5px;'>
![](/assets/images/lecture-feature_selection__dimensionality_reduction/image_20241212_052502_c1eb2af08e4644ebb8e7ffd85c6ef6e1.png)

</div>
<div class='column' style='flex: 1; padding: 0.5em; background-color: #f9f9f9; border: 1px solid #ddd; border-radius: 5px;'>
ì´ë ‡ê²Œ featureë¥¼ í•˜ë‚˜ì”© ì¶”ê°€í•œë‹¤.

</div>
</div>


## Backward-stepwise selection

<div class='column-list' style='display: flex; gap: 1em;'>
<div class='column' style='flex: 1; padding: 0.5em; background-color: #f9f9f9; border: 1px solid #ddd; border-radius: 5px;'>
![](/assets/images/lecture-feature_selection__dimensionality_reduction/image_20241212_052505_1f5850e9e602419d9d30f4b0c3328ff6.png)

</div>
<div class='column' style='flex: 1; padding: 0.5em; background-color: #f9f9f9; border: 1px solid #ddd; border-radius: 5px;'>
        - ì „ì²´ íŠ¹ì„±ì„ í¬í•¨í•œ full modelì—ì„œ í•˜ë‚˜ì”© ì œê±°í•œë‹¤
        - ê°€ì¥ ìœ ì˜ë¯¸í•˜ì§€ ì•Šì€ ë³€ìˆ˜ë¶€í„° ì œê±°

</div>
</div>


# Regularization

![](/assets/images/lecture-feature_selection__dimensionality_reduction/image_20241212_052506_ddc5b2a00f1e486586254bab94f1f244.png)

ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•´ ëª¨ë¸ ë³µì¡ì„±ì„ ìµœëŒ€í•œ ì¤„ì—¬ë³´ì.


## Ridge Regression (L2-ì •ê·œí™”)

- $$\beta$$ì˜ ì œê³±í•©ì— ëŒ€í•˜ì—¬ penalty ë¶€ì—¬
- $$\lambda â‰¥ 0$$ì´ ì •ê·œí™”ì˜ ìˆ˜ì¤€ì„ ê²°ì • â†’ $$\lambda$$ê°€ ì»¤ì§ˆìˆ˜ë¡ ì œì•½ì´ ë” ê°•í•˜ê²Œ ê±¸ë¦¼
- ë¹„êµì  ì‘ì€ $$\beta$$ë“¤ë¡œ ëª¨ë¸ì´ ê²°ì •ë¨
- Inputì— ëŒ€í•œ scalingì´ ìš°ì„ ì ìœ¼ë¡œ ì§„í–‰ë˜ì–´ì•¼ íš¨ê³¼ ë³¼ ìˆ˜ ìˆìŒ
![](/assets/images/lecture-feature_selection__dimensionality_reduction/image_20241212_052507_770d0df7ff474e94bb7ec8619e59756f.png)


## Lasso Regression (L1-ì •ê·œí™”)

- $$\beta$$ì˜ ì ˆëŒ“ê°’ í•©ì— ëŒ€í•˜ì—¬ penalty ë¶€ì—¬
- $$\lambdaâ‰¥0$$ì´ ì •ê·œí™”ì˜ ìˆ˜ì¤€ ê²°ì • â†’ $$\lambda$$ê°€ ì»¤ì§ˆìˆ˜ë¡ ì œì•½ì´ ë” ê°•í•˜ê²Œ ê±¸ë¦¼
- $$\beta$$ ì¤‘ 0ì´ ë§ì´ í¬í•¨ë˜ì–´ ëª¨ë¸ì´ ê²°ì •ë¨ (íŠ¹ì„± ì„ íƒì˜ íš¨ê³¼)
- Inputì— ëŒ€í•œ Scalingì´ ìš°ì„ ì ìœ¼ë¡œ ì§„í–‰ë˜ì–´ì•¼ íš¨ê³¼ ë³¼ ìˆ˜ ìˆìŒ
![](/assets/images/lecture-feature_selection__dimensionality_reduction/image_20241212_052509_8aa9d2d47b2d45aca870c38fe413476d.png)


## Ridge vs. Lasso

- Ridgeì˜ $$\beta$$ë“¤ì€ 0ê³¼ ê°€ê¹Œì›Œì§€ì§€ë§Œ 0ì´ ë˜ì§„ ì•ŠìŒ
- Lassoì˜ $$\beta$$ë“¤ì€ ì‹¤ì œ 0ìœ¼ë¡œ ë§ì´ êµ¬ì„±ë¨

# ì£¼ì„±ë¶„ ë¶„ì„; Principal Component Analysis


### ì°¨ì›ì˜ ì €ì£¼ (Curse of Dimensionality)

ì°¨ì›ì´ ì»¤ì§ˆìˆ˜ë¡ ê³µê°„ì´ ë„ˆë¬´ í¬ì†Œí•´ì§ â†’ ê³¼ì í•© ë°œìƒ

> ğŸ”´ ê³ ì°¨ì›ì˜ ë°ì´í„°ë¥¼ ë‚®ì€ ì°¨ì›ìœ¼ë¡œ í‘œí˜„
â†’ ë°ì´í„°ì˜ Varianceë¥¼ ìµœëŒ€í•œìœ¼ë¡œ ìœ ì§€í•  ìˆ˜ ìˆëŠ” ì¶•ë“¤ì„ ì°¾ì•„ë³´ì


### ì •ì‚¬ì˜(projection) ë‹¤ì‹œ ìƒê°í•´ë³´ê¸°

![](/assets/images/lecture-feature_selection__dimensionality_reduction/image_20241212_052510_fdfa5487019c4b15af90910a2d1b8b84.png)

![](/assets/images/lecture-feature_selection__dimensionality_reduction/image_20241212_052511_ba4533c2211b476298d8d2c12d4e05cd.png)

$$b$$**ì˜ ê¸¸ì´ê°€ 1ì´ë©´ **$$a$$**ì™€ **$$b$$**ì˜ ë‚´ì  **$$a^Tb$$**ì´ **$$a$$**ë¥¼ **$$b$$**ë¼ëŠ” ì¶•ìœ¼ë¡œ ì •ì‚¬ì˜ì‹œí‚¨ ì¢Œí‘œë¥¼ ê²°ì •í•œë‹¤.**


## Maximum variance principal component

- $$v_1, v_2, \cdots, v_p$$ë¥¼ $$p$$ê°œì˜ principal componentë¼ê³  í•˜ì.
- Data $$X$$ëŠ” ëª¨ë‘ í‰ê· =0 ìœ¼ë¡œ centering ë˜ì–´ ìˆìŒ
- Sample Varianceë¥¼ maximizeí•˜ëŠ” ë°©í–¥ $$v$$ ì°¾ê¸°
ì»¬ëŸ¼ ë³„ë¡œ í‰ê·  ë‚´ì–´ í¸ì°¨ í–‰ë ¬ ìƒì„±

![](/assets/images/lecture-feature_selection__dimensionality_reduction/image_20241212_052512_2882ea7d5664462ab9dcc283416cceaf.png)


